----------------------------------
FRAGEN
----------------------------------

- Wieviele Daten insgesamt genau nehmen? Hälfte?

- Wie viele Talibs pro Klasse?

- Frage in Multi-Input_LSTM Zeile 107?

- LSTM auf LSTM -> Deswegen im ersten Sequenz weitergeben? Ende der forward Methode?

- Zusammenhang -> Netzwerk - Architektur-Zeichnung und LSTM TaLib-Klassen Input <-> Wiedersprüchlich?

- TaLib Funktionen erwarten -> Sequenz -> Pro Chunk der Größe t also nur ein Wert berechnen je Talib Klasse?
-> Sieht Datenstruktur dann so aus?:
[ [Liste 22x5 (Open,Close,Volume], [BBANDS], [SMA], [MOM] ... ]
bzw. auf Zeichnung
[ [Y], [BBANDS], [SMA], [MOM] ... ]

- Frage in Multi-Input_LSTM Zeile 107?

- Next steps -> CUSTOM_LSTM mit separaten Gates anpassen?

- Basierend auf dem Netzwerk-Architekturdiagramm: Kann alles so bleiben, nur jede Talib einzelnes Gate?

Neu:
- Entire_model.py -> Z.22: Why 1 here?; Why same hidden size each here? Hidden size hier 64? Kann ich hidden_size beliebig variieren?
- Entire_model.py -> Z.71: What to do with batch size here?
- CustomMultiInput_LSTM -> Z11: Bild in Paper: Alles gleich nur separate Input Gates?
- CustomMultiInput_LSTM -> Z12: Erstes 'Gate' ohne Inputs, was auch in den Attention Layer fliesst -> fliesst ja Y ein -> bleibt gleich bei uns?
- CustomMultiInput_LSTM -> Z122: Verständnis -> was ist das? nicht im Schaubild abgebildet
- CustomMultiInput_LSTM -> Z141: Datenstruktur hier? (vorher dreidimensional gewesen?) -> siehe Zeile 97; Y_t = einzelner Record mit den 5 columns?
- CustomMultiInput_LSTM -> Z182: Ist das der Punkt in Architekur Grafik? 'Carry State'?
- CustomMultiInput_LSTM -> Z193: Wo ist das in Architekur Grafik?
- CustomMultiInput_LSTM -> Z207: Zwischenschritt / Attention Layer?
- CustomMultiInput_LSTM -> Z227: Wieso Weitergabe der Sequenz -> das ist Wesen eines MultiInput LSTMs?
- CustomMultiInput_LSTM -> Z238: Wo kommt diese Klasse vom Attention Layer ins Spiel?

Fragen_Neu_2:
- CustomMultiInput_LSTM -> Z11: Bild in Paper: Alles gleich nur separate Input Gates?
- Wichtige Punkte durchgehen ab EntireModel unten
- Kann man die Zahl der Input Size einfach varrieren über Q?
- Was noch machen?

----------------------------------
ANTWORTEN
----------------------------------

- Soviele Daten nehmen wie geht

- Einfach festlegen die Anzahl Talibs pro Klasse (nicht so wichtig)

- Ja Talib auch normalisieren

- Ja, zwei unterschiedliche LSTMs -> erste ist ein MultiInput LSTM mit Inputs: (22x5), (Talib-Klasse1), (Talib-Klasse2) .. (Talib-Klasse10) [bei 10 Klassen]

- Input sieht so aus [ [Liste 22x5 (Open,Close,Volume, Min, Max], (Talib-Klasse1), (Talib-Klasse2) ... ]

- Es gibt ein Talib-Wert für jeden einzelnen Tag (das wird berechnet z.B. 14 Tage in die Vergangenheit) -> deswegen muss man bei Generate_Samples nochmal zurückrechnen (erster Wert aus dem Input-Array errechnet sich aus den letzten 14 Tagen der Vergangenheit)

- Hidden Sequence Output aus erstem Multi Input LSTM wird Input vom zweiten Custom LSTM

- Entire Model Z113: Hier InitalInput [Liste 22x5 (Open,Close,Volume, Min, Max], (Talib-Klasse1), (Talib-Klasse2)]-> bei 10 Talib Classes hätte ich hier 11 Parameter